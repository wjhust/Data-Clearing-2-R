---
title: "线性模型引论—第六章程序题"
author: "王健"
date: "2017/6/6"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 线性模型引论-6.6题

```{r}
x1<-c(81.2,82.9,83.2,85.9,88,99,102,105.3,117.7,126.4,131.2,148,153,161,170,174,185,189)
x2<-c(85,92
      ,91.5
      ,92.9
      ,93
      ,96
      ,95
      ,95.6
      ,98.9
      ,101.5
      ,102
      ,105
      ,106
      ,109
      ,112
      ,112.5
      ,113
      ,114)
x3<-c(87
      ,94
      ,95
      ,95.5
      ,96
      ,97
      ,97.5
      ,98
      ,101.2
      ,102.5
      ,104
      ,105.9
      ,111
      ,110
      ,112
      ,112.3
      ,96
      ,113)
y<-c(7.8
     ,8.4
     ,8.7
     ,9
     ,9.6
     ,10.3
     ,10.6
     ,10.9
     ,11.3
     ,12.3
     ,13.5
     ,14.2
     ,14.9
     ,15.9
     ,18.5
     ,19.5
     ,19.9
     ,20.5)
fm<-lm(y~x1+x2+x3)
#建立回归模型
fm
summary(fm)
#查看回归情况
tstep<-step(fm)
summary(tstep)
#逐步回归，已经是最优模型,选模型和原模型一致，参数q=3;


AIC(fm)
#按照公式计算
y<-deviance(fm)
y
aic=18*log(y)+2*3
aic
#计算aic的数值



y1<-deviance(fm)
RMS=y1/(18-3)
RMS
#计算残差平方和RMS，由于和选模型一致，因此只需除以3即为平均残差平方和准则（RMS)
#选模型的确定使用逐步回归，如果选模型和原模型不同，直接计算选模型的RMS






#y=-10.43215+7.72623*x1-1.135*x2+0.1984*x3
y=y
y
a=x1
a
b=x2
c=x3
e=y-(-10.43215+0.07386*a+0.19803*b-0.05713*c)
e
w=matrix(e)
w
tw=t(w)
tw
q=tw%*%w
q
Cp=y1/q-(18-2*3)
Cp
#计算Cp统计量的数值
#综上：Cp=-11；RMS=0.45；AIC=443.74


```


# 线性模型引论-6.7题
```{r}
x1<-c(8.3,8.6,8.8,10.5,10.7,10.8,11.0,11.0,11.1,11.2,11.3,11.4,11.4,11.7,12.0,12.0,12.9,13.3,13.7,13.8,14.0,14.2,14.5,13,16.3,17.3,17.5,17.9,18.0,18.0,20.6)
length(x1)
x2<-c(70,65,63,72,81,83,66,75,80,75,79,76,76,69,75,74,85,86,71,64,78,80,74,72,77,81,82,80,80,80,87)
length(x2)
y<-c(10.3,10.3,10.2,16.4,18.8,19.7,15.6,18.2,22.6,19.9,24.2,21.0,21.4,21.3,19.1,22.2,33.8,27.4,25.7,24.9,34.5,31.7,36.3,38.3,42.6,55.4,55.7,58.3,51.5,51,77)
length(y)
#（1）
fm<-lm(y~(x1^2)+x2)
#建立回归模型
fm
summary(fm)
#查看回归情况
plot(fm)
#画出回归分析图，qq图，残差分析图等
#（2）进行boxcox变换
library(MASS)
boxcox(y~(x1^2)+x2)
#进行boxcox变换
boxcox(y~(x1^2)+x2,lambda = seq(0.1,0.65,length=10))
#参数选择和进一步回归
y<-(y^(0.38-1))/0.38
t<-lm(y~(x1^2)+x2)
#变换后进行回归，并作图像
plot(t)
```

# 线性模型引论-6.11题岭回归，共线性
```{r}
y<-c(16.3,16.8,19.2,18.0,19.5,20.9,21.1,20.9,20.3,22.0)
length(y)
x1<-c(1.0,1.4,1.7,1.7,1.8,1.8,1.9,2.0,2.3,2.4)
length(x1)
x2<-c(1.1,1.5,1.8,1.7,1.9,1.8,1.8,2.1,2.4,2.5)
length(x2)
#（1）
x<-cbind(x1,x2)
x=as.matrix(x)
x
z=t(x)
z
c=z%*%x
c
qr(x)$rank
eigen(c)
#判断共线性，计算k值
k=69.95/0.02
k
#k>1000，因此有严重的复共线性
library(MASS)
fm<-lm.ridge(y~x1+x2,lambda = seq(0,150,length=151))
#建立岭回归模型，选择参数
summary(fm)
#查看回归情况
print(fm)
#作图
plot(lm.ridge(y~x1+x2,lambda = seq(0,150,length=151)))
#通过岭迹图判断大概k=50之后稳定下来，取k=50
fm<-lm.ridge(y~x1+x2,lambda = 50)
#作图后找到合适参数，对模型修参数，重新回归
summary(fm)
#查看回归情况
fm

#（2）按照定义计算所求
d=diag(2)
d
f=(c+50*d)
f
g=solve(f,diag(2))
h=g%*%z%*%y
h
mean(x1)
mean(x2)
mean(y)
sd(x1)
sd(x2)
sd(y)
```

# 线性模型引论-6.12回归以及主成分分析
```{r}
x1<-c(82.9,88.0,99.9,15.3,117.7,131,148.2,161.8,174.2,184.7)
length(x1)
x2<-c(92,93,96,94,100,101,105,112,112,112)
length(x2)
x3<-c(17,21.3,25.1,29,34,40,44,49,51,53)
length(x3)
x4<-c(94,96,97,97,100,101,104,109,111,111)
length(x4)
y<-c(8.4,9.6,10.4,11.4,12.2,14.2,15.8,17.9,19.6,20.8)
length(y)
df<-data.frame(x1,x2,x3,x4,y)
df
#（1）主成分分析
a<-princomp(~x1+x2+x3+x4,data=df,cor=T)
summary(a,loadings = TRUE)
#根据结果和p值，去掉第三，四成分
#主成分回归
pre<-predict(a)
pre
df$z1<-pre[,1]
df$z2<-pre[,2]
fm<-lm(y~z1+z2,data=df)
#建立回归模型
summary(fm)
#检查回归情况
beta<-coef(fm); A<-loadings(a)
x.bar<-a$center; x.sd<-a$scale
coef<-(beta[2]*A[,1]+ beta[3]*A[,2])/x.sd
beta0 <- beta[1]- sum(x.bar * coef)
c(beta0, coef)
#对结果进行转换，按照书中公示转化为所求


```





